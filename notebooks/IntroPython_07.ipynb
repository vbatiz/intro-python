{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKWXreTj95Z6"
      },
      "source": [
        "# Complejidad de los datos\n",
        "\n",
        "### Introducci√≥n\n",
        "\n",
        "En esta sesi√≥n analizaremos el tema de complejidad de los datos y algunas t√©cnicas para tratar los efectos de la misma. Este cuaderno se basa parcialmente en el material del curso de limpieza de datos de Kaggle disponible [aqu√≠](https://www.kaggle.com/learn/data-cleaning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8qKe50s95Z-"
      },
      "source": [
        "### Gesti√≥n de valores omitidos\n",
        "Elimine los valores que faltan o rell√©nelos con un flujo de trabajo automatizado.\n",
        "\n",
        "La limpieza de datos es una parte clave de la ciencia de datos, pero puede ser muy frustrante. ¬øPor qu√© hay campos de texto ilegibles? ¬øQu√© hacer con los valores que faltan? ¬øPor qu√© las fechas no tienen el formato correcto? ¬øC√≥mo puede solucionar r√°pidamente la introducci√≥n de datos incoherentes? En este tema, aprender√° por qu√© se ha encontrado con estos problemas y, lo que es m√°s importante, c√≥mo solucionarlos.\n",
        "\n",
        "En este cuaderno, aprender√° a abordar algunos de los problemas m√°s comunes de limpieza de datos para que pueda analizar sus datos m√°s r√°pidamente. Realizar√° cinco ejercicios pr√°cticos con datos reales y desordenados y responder√° a algunas de las preguntas m√°s frecuentes sobre la limpieza de datos.\n",
        "\n",
        "En este cuaderno, veremos c√≥mo tratar los valores faltantes u omitidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bySSHnJZ95Z-"
      },
      "source": [
        "#### Primer vistazo a los datos\n",
        "\n",
        "Lo primero que tenemos que hacer es cargar las bibliotecas y el conjunto de datos que vamos a utilizar.\n",
        "\n",
        "Para la demostraci√≥n, utilizaremos un conjunto de datos de eventos ocurridos en partidos de f√∫tbol americano. Debido al tama√±o del conjunto de datos, lo descargaremos y posteriormente lo cargaremos a nuestro espacio temporal. [Ir a la p√°gina de descarga](https://www.kaggle.com/code/alexisbcook/handling-missing-values/data?select=NFL+Play+by+Play+2009-2017+%28v4%29.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOMBSNG095Z_",
        "outputId": "4d6bd9f6-3029-4cf4-9381-164958c6442c"
      },
      "outputs": [],
      "source": [
        "# m√≥dulos que usaremos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# cargamos los datos\n",
        "nfl_data = pd.read_csv(\"NFL Play by Play 2009-2017 (v4).csv\")\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEI_X99_95aA"
      },
      "source": [
        "Lo primero que hay que hacer cuando se recibe un nuevo conjunto de datos es echar un vistazo a algunos de ellos. Esto nos permite ver que todo se lee correctamente y nos da una idea de lo que est√° pasando con los datos. En este caso, vamos a ver si hay valores perdidos u omitidos, que son representados en Python con `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "ILNIu14E95aA",
        "outputId": "8be11832-50fc-400b-94b9-1fa8fec620ab"
      },
      "outputs": [],
      "source": [
        "nfl_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_W6oH2695aB"
      },
      "source": [
        "¬øObservamos datos faltantes?\n",
        "\n",
        "¬øCu√°ntos puntos de datos faltantes tenemos?\n",
        "\n",
        "Bien, ahora sabemos que tenemos algunos valores faltantes. Veamos cu√°ntos tenemos en cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DrU9z_795aB",
        "outputId": "45b3c008-7475-4724-9ed9-e63a32bc7606"
      },
      "outputs": [],
      "source": [
        "# obtenemos el n√∫mero de datos faltantes por columna\n",
        "missing_values_count = nfl_data.isnull().sum()\n",
        "\n",
        "# Revisamos el n√∫mero de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
        "missing_values_count[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pVqW_Qr95aC"
      },
      "source": [
        "¬øQu√© opinas de los resultados mostrados? Ser√≠a √∫til saber qu√© porcentaje de valores faltan en nuestro conjunto de datos para hacernos una idea m√°s precisa de la magnitud del problema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gACtEoRaOguR",
        "outputId": "9b8e94cf-f429-4b54-e169-35ac6bad254e"
      },
      "outputs": [],
      "source": [
        "print(nfl_data.shape)\n",
        "a,b = nfl_data.shape\n",
        "print(a)\n",
        "print(b)\n",
        "print(a*b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5EXLnNs95aC",
        "outputId": "0b005414-9fa0-40f0-e559-9e2cffc6d87e"
      },
      "outputs": [],
      "source": [
        "# ¬øCu√°ntos valores faltantes tenemos en total en el conjunto datos?\n",
        "print(nfl_data.shape)\n",
        "total_cells = np.product(nfl_data.shape)\n",
        "total_missing = missing_values_count.sum()\n",
        "\n",
        "# porcentaje de datos faltante\n",
        "percent_missing = (total_missing/total_cells) * 100\n",
        "print(f'Celdas totales: {total_cells:,}')   # Se agrega :, a la derecha de la variable para dar formato de miles\n",
        "print(f'Celdas con datos faltantes: {total_missing:,}') # Se agrega :, a la derecha de la variable para dar formato de miles\n",
        "print(f'Porcentaje de datos faltantes: {round(percent_missing,2)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWLqWpQ995aC"
      },
      "source": [
        "¬øQu√© opinas del porcentaje de datos faltantes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGtzGiAz95aD"
      },
      "source": [
        "#### Averiguar por qu√© faltan datos\n",
        "\n",
        "Este es el punto en el que entramos en la parte de la ciencia de datos que solemos llamar \"intuici√≥n de datos\", es decir, \"analizar realmente los datos e intentar averiguar por qu√© son como son y c√≥mo afectar√°n a nuestro an√°lisis\". Puede ser una parte frustrante de la ciencia de datos, especialmente si eres nuevo en este campo y no tienes mucha experiencia. Para tratar los valores que faltan, tendr√°s que usar tu intuici√≥n para averiguar por qu√© falta el valor. Una de las preguntas m√°s importantes que puede hacerse para averiguarlo es la siguiente:\n",
        "\n",
        "**¬øEste valor falta porque no se registr√≥ o porque no existe?**\n",
        "\n",
        "Si falta un valor porque no existe (como la altura del hijo mayor de alguien que no tiene hijos), no tiene sentido intentar adivinar cu√°l podr√≠a ser. Estos valores probablemente quieras mantenerlos como NaN. Por otro lado, si falta un valor porque no se registr√≥, puede intentar adivinar cu√°l podr√≠a haber sido bas√°ndose en los dem√°s valores de esa columna y fila. Esto se llama imputaci√≥n, ¬°y aprenderemos a hacerlo a continuaci√≥n! :)\n",
        "\n",
        "Veamos un ejemplo. Observando el n√∫mero de valores faltantes en el marco de datos nfl_data, nos damos cuenta de que la columna \"TimeSecs\" tiene muchos valores faltantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPBHFJTf95aD",
        "outputId": "acb9e667-0977-432f-f05e-25413fa0384a"
      },
      "outputs": [],
      "source": [
        "# Revisamos el n√∫mero de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
        "missing_values_count[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR_eAYIh95aD"
      },
      "source": [
        "Revisando la documentaci√≥n, podemos ver que esta columna tiene informaci√≥n sobre el n√∫mero de segundos que quedaban en el partido cuando se hizo la jugada. Esto significa que estos valores probablemente faltan porque no se registraron, y no porque no existan. Por lo tanto, tendr√≠a sentido que intent√°ramos adivinar cu√°les deber√≠an ser en lugar de dejarlos como `NaN`.\n",
        "\n",
        "Por otra parte, hay otros campos, como \"PenalizedTeam\", en los que tambi√©n faltan muchos campos. En este caso, sin embargo, el campo falta porque si no hubo penalizaci√≥n no tiene sentido decir qu√© equipo fue penalizado. Para esta columna, tendr√≠a m√°s sentido dejarla vac√≠a o a√±adir un tercer valor como \"ninguno\" y utilizarlo para reemplazar los `NaN`.\n",
        "\n",
        "Si est√° realizando un an√°lisis de datos muy cuidadoso, este es el punto en el que mirar√≠a cada columna individualmente para averiguar cu√°l es la mejor estrategia para rellenar los valores que faltan. En el resto de este cuaderno, trataremos algunas t√©cnicas \"r√°pidas y sucias\" que pueden ayudarle con los valores que faltan, pero que probablemente acabar√°n eliminando informaci√≥n √∫til o a√±adiendo ruido a los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8bNN-dA95aD"
      },
      "source": [
        "#### Eliminar valores faltantes\n",
        "Si tiene prisa o no tiene motivos para averiguar por qu√© faltan valores, una opci√≥n es eliminar las filas o columnas que contengan valores que falten. (Nota: ¬°generalmente no se recomienda este enfoque para proyectos importantes! Suele merecer la pena tomarse el tiempo necesario para revisar los datos y examinar una por una todas las columnas con valores faltantes para conocer realmente el conjunto de datos).\n",
        "\n",
        "Si est√° seguro de que quiere eliminar las filas con valores faltantes, pandas tiene una funci√≥n muy √∫til, dropna() para ayudarle a hacerlo. Vamos a probarla en nuestro conjunto de datos de la NFL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "aCC-LbXn95aD",
        "outputId": "3530f173-a45e-4a24-8ac4-282239c38f05"
      },
      "outputs": [],
      "source": [
        "# eliminar todos los renglones que contengan un valor faltante u omitido\n",
        "nfl_data.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU52YwaO95aD"
      },
      "source": [
        "¬øQu√© sucedi√≥? ¬°parece que se han eliminado todos nuestros datos! üò± Esto se debe a que cada fila de nuestro conjunto de datos ten√≠a al menos un valor faltante. Podr√≠amos tener mejor suerte eliminando todas las columnas que tienen al menos un valor faltante en su lugar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "MTSMg6Oh95aE",
        "outputId": "28fde50b-4da4-4707-d2c4-d705437f4281"
      },
      "outputs": [],
      "source": [
        "# Eliminemos ahora todas las columnas en donde exista un valor faltante\n",
        "columns_with_na_dropped = nfl_data.dropna(axis=1) #Al agregar el par√°metro axis=1 estamos indicando que el criterio sea revisar columnas. Por defecto se revisan renglones (axis=0)\n",
        "columns_with_na_dropped.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F9Zx2fy95aE",
        "outputId": "78cde983-37d2-4c2d-f1b9-a82c2c3d376d"
      },
      "outputs": [],
      "source": [
        "columns_with_na_dropped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDbH8ctBTFuM",
        "outputId": "d859dd22-6a64-491f-c226-b44e930a566d"
      },
      "outputs": [],
      "source": [
        "#a, b = nfl_data.shape\n",
        "a = nfl_data.shape[0]\n",
        "b = nfl_data.shape[1]\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbmwdhos95aE"
      },
      "source": [
        "Calcular cuanta informaci√≥n hemos perdido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6gO59CL95aE",
        "outputId": "91d408be-1f67-4a10-88c9-d3b030d32ec8"
      },
      "outputs": [],
      "source": [
        "print(f\"Columnas en el conjunto de datos original: {nfl_data.shape[1]}\")\n",
        "print(f\"Columnas restantes tras eliminar datos faltantes: {columns_with_na_dropped.shape[1]}\")\n",
        "print(f\"Total de columnas eliminadas: {nfl_data.shape[1] - columns_with_na_dropped.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoL3EIta95aE"
      },
      "source": [
        "#### Rellenar autom√°ticamente los valores que faltan\n",
        "Otra opci√≥n es intentar rellenar los valores que faltan. Para ello, vamos a tomar una peque√±a subsecci√≥n de los datos de la NFL para que se imprima bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "_BMKzKX_95aF",
        "outputId": "5639d070-ece2-4a87-cb99-4dae3a7cf68b"
      },
      "outputs": [],
      "source": [
        "# obtenemos un peque√±o subconjunto del conjunto de datos de la NFL\n",
        "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\n",
        "subset_nfl_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Jw-Jfa95aF"
      },
      "source": [
        "Podemos utilizar la funci√≥n fillna() de Panda para que rellene por nosotros los valores que faltan en un marco de datos. Una opci√≥n que tenemos es especificar con qu√© queremos que se sustituyan los valores NaN. Aqu√≠, estoy diciendo que me gustar√≠a reemplazar todos los valores NaN con 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "S2Hbb2AS95aF",
        "outputId": "017d3638-ba64-4579-f108-9800201709db"
      },
      "outputs": [],
      "source": [
        "# remplazar todos los datos NaN con 0\n",
        "subset_nfl_data.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_D13Epg95aF"
      },
      "source": [
        "Quiz√°s una mejor estrategia sea sustituir los valores que faltan por cualquier valor que le siga directamente en la misma columna. (Esto tiene mucho sentido para conjuntos de datos en los que las observaciones tienen alg√∫n tipo de orden l√≥gico)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Eb6YJfNg95aF",
        "outputId": "bed43cc5-1fac-44bc-9e4c-73698bb84008"
      },
      "outputs": [],
      "source": [
        "# reemplazar todos los NaN con el valor que viene directamente despu√©s de √©l en la misma columna\n",
        "# y sustituir todos los NaN restantes por 0\n",
        "subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXpxJVHB95aF"
      },
      "source": [
        "Opciones para el par√°metro `method`:\n",
        "\n",
        "**ffill**\n",
        "\n",
        "Rellenar valores propagando la √∫ltima observaci√≥n v√°lida a la siguiente v√°lida.\n",
        "\n",
        "**bfill**\n",
        "\n",
        "Rellenar valores utilizando la siguiente observaci√≥n v√°lida para rellenar el hueco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "2YeFf_XYVjPr",
        "outputId": "93390419-f384-44bf-d11d-82ce3ac3cd5f"
      },
      "outputs": [],
      "source": [
        "# reemplazar todos los NaN con el valor que viene directamente antes de √©l en la misma columna\n",
        "# y sustituir todos los NaN restantes por 0\n",
        "subset_nfl_data.fillna(method='ffill', axis=0).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7QsTC8M95aF"
      },
      "source": [
        "### Escalado y Normalizaci√≥n\n",
        "Transformar variables num√©ricas para que tengan propiedades √∫tiles.\n",
        "\n",
        "#### Cargamos los m√≥dulos a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvoJILGl95aF",
        "outputId": "bf9a7e3b-ef35-4dbe-f51a-6a0624eb528b"
      },
      "outputs": [],
      "source": [
        "%pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-apKtOUV95aG"
      },
      "outputs": [],
      "source": [
        "# m√≥dulos a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# para transformaci√≥n Box-Cox\n",
        "from scipy import stats\n",
        "\n",
        "# para escalado min_max\n",
        "from mlxtend.preprocessing import minmax_scaling\n",
        "\n",
        "# m√≥dulos de visualizaci√≥n\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcTUe-d095aG"
      },
      "source": [
        "#### Escalado frente a normalizaci√≥n: ¬øCu√°l es la diferencia?\n",
        "\n",
        "Una de las razones por las que es f√°cil confundirse entre escalado y normalizaci√≥n es porque los t√©rminos a veces se utilizan indistintamente y, para hacerlo a√∫n m√°s confuso, ¬°son muy similares! En ambos casos, se transforman los valores de las variables num√©ricas para que los puntos de datos transformados tengan propiedades √∫tiles espec√≠ficas. La diferencia es que en el escalado, se cambia el rango de los datos, mientras que en la normalizaci√≥n, cambia la forma de la distribuci√≥n de los datos.\n",
        "\n",
        "Hablemos un poco m√°s en profundidad de cada una de estas opciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_34rZS195aG"
      },
      "source": [
        "#### Escalado\n",
        "\n",
        "Esto significa que est√° transformando sus datos para que se ajusten a una escala espec√≠fica, como 0-100 o 0-1. Es conveniente escalar los datos cuando se utilizan m√©todos basados en medidas de la distancia entre los puntos de datos, como las m√°quinas de vectores de soporte (SVM) o los vecinos m√°s cercanos (KNN). Con estos algoritmos, un cambio de \"1\" en cualquier caracter√≠stica num√©rica recibe la misma importancia.\n",
        "\n",
        "Por ejemplo, puede consultar los precios de algunos productos en yenes y en d√≥lares estadounidenses. Un d√≥lar estadounidense vale unos 100 yenes, pero si no escala los precios, m√©todos como SVM o KNN considerar√°n que una diferencia de precio de 1 yen es tan importante como una diferencia de 1 d√≥lar estadounidense. Est√° claro que esto no encaja con nuestras intuiciones del mundo. Con la divisa, puedes convertir entre divisas. Pero, ¬øqu√© ocurre con la altura y el peso? No est√° del todo claro cu√°ntas libras equivalen a una pulgada (o cu√°ntos kilogramos equivalen a un metro).\n",
        "\n",
        "Al escalar las variables, puedes comparar diferentes variables en igualdad de condiciones. Para ayudarte a entender c√≥mo es el escalado, veamos un ejemplo inventado. (No te preocupes, en el siguiente ejercicio trabajaremos con datos reales)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "SnuPh2SE95aG",
        "outputId": "c11b5f75-211c-4c24-ac8f-a38900a29ad0"
      },
      "outputs": [],
      "source": [
        "# generar 1000 puntos de datos extra√≠dos aleatoriamente de una distribuci√≥n exponencial\n",
        "original_data = np.random.exponential(size=1000)\n",
        "\n",
        "# mix-max escala los datos entre 0 y 1\n",
        "scaled_data = minmax_scaling(original_data, columns=[0])\n",
        "\n",
        "# graficamos ambos para comparar\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
        "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
        "ax[0].set_title(\"Datos Originales\")\n",
        "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
        "ax[1].set_title(\"Datos Escalados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIN8liZR95aG"
      },
      "source": [
        "Observe que la forma de los datos no cambia, pero que en lugar de ir de 0 a 8, ahora van de 0 a 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7oeRww95aG"
      },
      "source": [
        "#### Normalizaci√≥n\n",
        "El escalado s√≥lo cambia el rango de los datos. La normalizaci√≥n es una transformaci√≥n m√°s radical. El objetivo de la normalizaci√≥n es cambiar las observaciones para que puedan describirse como una distribuci√≥n normal.\n",
        "\n",
        "[Distribuci√≥n normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal): Tambi√©n conocida como \"curva de campana\", es una distribuci√≥n estad√≠stica espec√≠fica en la que aproximadamente el mismo n√∫mero de observaciones se sit√∫an por encima y por debajo de la media, la media y la mediana son iguales y hay m√°s observaciones cerca de la media. La distribuci√≥n normal tambi√©n se conoce como distribuci√≥n de Gauss.\n",
        "\n",
        "En general, normalizar√° sus datos si va a utilizar una t√©cnica de aprendizaje autom√°tico o estad√≠stica que asuma que sus datos se distribuyen normalmente. Algunos ejemplos son el an√°lisis discriminante lineal (LDA) y el Bayes ingenuo gaussiano. (Consejo profesional: cualquier m√©todo con \"gaussiano\" en el nombre probablemente asume la normalidad).\n",
        "\n",
        "El m√©todo que estamos utilizando para normalizar aqu√≠ se llama Transformaci√≥n [Box-Cox](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Echemos un vistazo r√°pido a c√≥mo se ve la normalizaci√≥n de algunos datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "nPk0p5VN95aG",
        "outputId": "da48ea57-5b2d-4000-8d4c-d4dbd79f711c"
      },
      "outputs": [],
      "source": [
        "# normalizar los datos exponenciales con boxcox\n",
        "normalized_data = stats.boxcox(original_data)\n",
        "\n",
        "# graficamos ambos para comparar\n",
        "fig, ax=plt.subplots(1, 2, figsize=(15, 3))\n",
        "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
        "ax[0].set_title(\"Datos Originales\")\n",
        "sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)\n",
        "ax[1].set_title(\"Datos Normalizados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DXNyKjD95aH"
      },
      "source": [
        "Observe que la forma de nuestros datos ha cambiado. Antes de la normalizaci√≥n ten√≠an casi forma de L. Pero despu√©s de la normalizaci√≥n se parecen m√°s al contorno de una campana (de ah√≠ lo de \"curva de campana\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lisis de fechas\n",
        "Ayuda a Python a reconocer fechas compuestas por d√≠a, mes y a√±o.\n",
        "\n",
        "#### Cargamos los m√≥dulos y conjunto de datos a utilizar\n",
        "Trabajaremos con un conjunto de datos que contiene informaci√≥n sobre los desprendimientos de tierra ocurridos entre 2007 y 2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# m√≥dulos a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "# cargamos los datos\n",
        "landslides_filepath = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/landslides.csv\"\n",
        "landslides = pd.read_csv(landslides_filepath)\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comprobar el tipo de datos de nuestra columna de fecha\n",
        "Empezaremos echando un vistazo a las cinco primeras filas de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landslides.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estaremos trabajando con la columna \"date\". Revisemos que realmente contenga fechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imprimir los primeros renglones de la columna \"date\"\n",
        "print(landslides['date'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al ver los datos podemos asumir como humanos que son datos de fechas, pero esto no significa que Python reconozca que son fechas. Si revisamos la √∫ltima l√≠nea de la funci√≥n head() notamos que el tipo de dato (dtype) es \"object\".\n",
        "\n",
        "Si revisamos la documentaci√≥n de dtype de pandas, veremos que tambi√©n hay un dtype espec√≠fico datetime64. Como el dtype de nuestra columna es object y no datetime64, podemos decir que Python no sabe que esta columna contiene fechas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi√©n podemos ver s√≥lo el dtype de una columna sin imprimir las primeras filas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podemos revisar el tipo de dato de una columna\n",
        "landslides['date'].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"O\" es el c√≥digo de \"objeto\", por lo que podemos ver que estos dos m√©todos nos dan la misma informaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tambi√©n podemos revisar los tipos de dato de todas las columnas\n",
        "landslides.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convertir nuestras columnas de fecha a datetime\n",
        "Ahora que sabemos que nuestra columna de fecha no est√° siendo reconocida como una fecha, es hora de convertirla para que sea reconocida como una fecha. Esto se llama \"analizar fechas\" (\"parsing dates\" en ingl√©s) porque estamos tomando una cadena e identificando las partes que la componen.\n",
        "\n",
        "Podemos determinar cu√°l es el formato de nuestras fechas con una gu√≠a llamada \"directiva strftime\", de la que puedes encontrar m√°s informaci√≥n [aqu√≠](https://strftime.org/)]. La idea b√°sica es que hay que se√±alar donde est√°n las diferentes partes de la fecha y qu√© signos de puntuaci√≥n hay entre ellas. Hay muchas partes posibles de una fecha, pero las m√°s comunes son %d para el d√≠a, %m para el mes, %y para un a√±o de dos d√≠gitos y %Y para un a√±o de cuatro d√≠gitos.\n",
        "\n",
        "Algunos ejemplos:\n",
        "\n",
        "- 1/17/07 tiene el formato \"%m/%d/%y\".\n",
        "- 17-1-2007 tiene el formato \"%d-%m-%Y\".\n",
        "\n",
        "Si volvemos a mirar la cabecera de la columna \"date\" (fecha) en el conjunto de datos de desprendimientos, vemos que tiene el formato \"month/day/two-digit year\" (mes/d√≠a/a√±o de dos d√≠gitos), por lo que podemos utilizar la misma sintaxis que en el primer ejemplo para analizar las fechas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crear una nueva columna, date_parsed, con las fechas analizadas\n",
        "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# revisamos los primeros datos\n",
        "landslides['date_parsed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que nuestras fechas est√°n correctamente analizadas, podemos interactuar con ellas de forma √∫til.\n",
        "\n",
        "¬øQu√© pasa si nos encontramos con un error con m√∫ltiples formatos de fecha? Mientras estamos especificando el formato de fecha, a veces se encontrar√° con un error cuando hay m√∫ltiples formatos de fecha en una sola columna. Si eso ocurre, puede hacer que pandas intente deducir cu√°l deber√≠a ser el formato de fecha correcto. Puede hacerlo as√≠:\n",
        "\n",
        "`landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)`\n",
        "\n",
        "¬øPor qu√© no utilizar siempre infer_datetime_format = True? Hay dos grandes razones para no hacer que pandas adivine siempre el formato de hora. La primera es que pandas no siempre ser√° capaz de averiguar el formato de fecha correcto, especialmente si alguien se ha puesto creativo con la entrada de datos. La segunda es que es mucho m√°s lento que especificar el formato exacto de las fechas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seleccionar el d√≠a del mes\n",
        "Ahora que tenemos una columna de fechas analizadas, podemos extraer informaci√≥n como el d√≠a del mes en que se produjo un desprendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtener e√± d√≠a del mes de la columna date_parsed\n",
        "day_of_month_landslides = landslides['date_parsed'].dt.day\n",
        "day_of_month_landslides.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¬øQu√© sucede si intentamos hacer lo mismo con la columna \"date\" original?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "day_lanslides = landslides['date'].dt.day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trazar el d√≠a del mes para comprobar el an√°lisis sint√°ctico de la fecha\n",
        "Uno de los mayores peligros al analizar fechas es confundir los meses y los d√≠as. La funci√≥n to_datetime() tiene mensajes de error muy √∫tiles, pero no est√° de m√°s volver a comprobar que los d√≠as del mes que hemos extra√≠do tienen sentido.\n",
        "\n",
        "Para ello, vamos a trazar un histograma de los d√≠as del mes. Esperamos que tenga valores entre 1 y 31 (Con un asterisco en el 31 porque no todos los meses tienen 31 d√≠as) y, puesto que no hay raz√≥n para suponer que los desprendimientos son m√°s frecuentes en unos d√≠as del mes que en otros, esperamos una distribuci√≥n relativamente uniforme. Veamos si es as√≠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remover valores nulos (NaN)\n",
        "day_of_month_landslides = day_of_month_landslides.dropna()\n",
        "\n",
        "# graficamos el d√≠a del mes\n",
        "sns.displot(day_of_month_landslides, kde=False, bins=31) #KDE (kernel density estimate) representa los datos mediante una curva de densidad de probabilidad continua en una o varias dimensiones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Codificaci√≥n de caracteres\n",
        "Evite errores de descodificaci√≥n Unicode al cargar archivos CSV.\n",
        "\n",
        "#### Cargamos los m√≥dulos que utilizaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# m√≥dulos b√°sicos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# m√≥dulo de codificaci√≥n de caracteres\n",
        "import charset_normalizer\n",
        "\n",
        "# fijar semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ¬øQu√© son las codificaciones?\n",
        "Las codificaciones de caracteres son conjuntos espec√≠ficos de reglas para pasar de cadenas binarias de bytes en bruto (que tienen este aspecto: 0110100001101001) a caracteres que forman un texto legible por humanos (como \"hola\"). Hay muchas codificaciones diferentes, y si intentas leer un texto con una codificaci√≥n diferente a la original, acabas con un texto confuso llamado \"mojibake\" (se dice mo-gee-bah-kay). He aqu√≠ un ejemplo de mojibake:\n",
        "\n",
        "\n",
        "√¶-‚Ä°√•-√•≈í-√£??\n",
        "\n",
        "\n",
        "Tambi√©n puedes encontrarte con caracteres \"desconocidos\". Estos son los que se imprimen cuando no hay correspondencia entre un byte en particular y un caracter en la codificaci√≥n que est√°s usando para leer tu cadena de bytes y se ven as√≠:\n",
        "\n",
        "\n",
        "ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n",
        "\n",
        "\n",
        "Los desajustes en la codificaci√≥n de caracteres son menos comunes hoy en d√≠a de lo que sol√≠an ser, pero definitivamente siguen siendo un problema. Hay muchas codificaciones de caracteres diferentes, pero la principal que necesitas conocer es UTF-8.\n",
        "\n",
        "\n",
        "- UTF-8 es la codificaci√≥n de texto est√°ndar. Todo el c√≥digo Python est√° en UTF-8 e, idealmente, todos tus datos deber√≠an estarlo tambi√©n. Es cuando las cosas no est√°n en UTF-8 cuando tienes problemas.\n",
        "\n",
        "\n",
        "Era bastante dif√≠cil tratar con codificaciones en Python 2, pero afortunadamente en Python 3 es mucho m√°s simple. (Los cuadernos Kaggle s√≥lo usan Python 3.) Hay dos tipos de datos principales que encontrar√°s cuando trabajes con texto en Python 3. Uno es `string`, que es lo que el texto es por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iniciamos con un string\n",
        "before = \"This is the euro symbol: ‚Ç¨\"\n",
        "\n",
        "# verificamos el tipo de dato\n",
        "type(before)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El otro dato es el tipo `bytes`, que es una secuencia de enteros. Puedes convertir una cadena en bytes especificando en qu√© codificaci√≥n est√°:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# codificarlo a una codificaci√≥n diferente, sustituyendo los caracteres que provocan errores\n",
        "after = before.encode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "# check the type\n",
        "print(type(after))\n",
        "print(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si revisas un objeto `bytes`, ver√°s que tiene una b delante, y quiz√°s algo de texto despu√©s. Esto se debe a que los bytes se imprimen como si fueran caracteres codificados en ASCII. (ASCII es una codificaci√≥n de caracteres antigua que realmente no funciona para escribir otro idioma que no sea el ingl√©s). Aqu√≠ puedes ver que nuestro s√≠mbolo del euro ha sido reemplazado por alg√∫n mojibake que parece \"\\xe2\\x82\\xac\" cuando se imprime como si fuera una cadena ASCII."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando volvemos a convertir nuestros bytes en una cadena con la codificaci√≥n correcta, podemos ver que nuestro texto est√° todo correctamente, ¬°lo cual es genial! :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convertirlo de nuevo a utf-8\n",
        "print(after.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, cuando intentamos utilizar una codificaci√≥n diferente para convertir nuestros bytes en una cadena, obtenemos un error. Esto se debe a que la codificaci√≥n que estamos intentando utilizar no sabe qu√© hacer con los bytes que estamos intentando pasarle. Necesitas decirle a Python la codificaci√≥n en la que se supone que debe estar la cadena de bytes.\n",
        "\n",
        "Puedes pensar en diferentes codificaciones como diferentes formas de grabar m√∫sica. Puedes grabar la misma m√∫sica en un CD, en una cinta de casete o en una de 8 pistas. Aunque la m√∫sica suene m√°s o menos igual, hay que utilizar el equipo adecuado para reproducir la m√∫sica de cada formato de grabaci√≥n. El descodificador correcto es como un reproductor de casetes o de CD. Si intentas reproducir un casete en un reproductor de CD, no funcionar√°."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# intenta decodificarlo con codificaci√≥n ascii\n",
        "print(after.decode(\"ascii\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi√©n podemos tener problemas si intentamos usar la codificaci√≥n incorrecta para pasar de una cadena a bytes. Como dije antes, las cadenas son UTF-8 por defecto en Python 3, as√≠ que si intentamos tratarlas como si estuvieran en otra codificaci√≥n crearemos problemas.\n",
        "\n",
        "Por ejemplo, si intentamos convertir una cadena a bytes para ASCII usando encode(), podemos pedir que los bytes sean los que ser√≠an si el texto estuviera en ASCII. Sin embargo, como nuestro texto no est√° en ASCII, habr√° algunos caracteres que no podr√° manejar. Podemos reemplazar autom√°ticamente los caracteres que ASCII no puede manejar. Sin embargo, si hacemos eso, cualquier car√°cter que no est√© en ASCII ser√° reemplazado por el car√°cter desconocido. Entonces, cuando convirtamos los bytes de nuevo en una cadena, el car√°cter ser√° reemplazado por el car√°cter desconocido. La parte peligrosa de esto es que no hay forma de saber qu√© car√°cter deber√≠a haber sido. Esto significa que podemos haber convertido nuestros datos en algo inusable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iniciamos con un string\n",
        "before = \"This is the euro symbol: ‚Ç¨\"\n",
        "\n",
        "# lo codificamos a un codificaci√≥n diferente, remplazando los caracteres que marquen error\n",
        "after = before.encode(\"ascii\", errors = \"replace\")\n",
        "\n",
        "# lo convertimos de nuevo a utf-8\n",
        "print(after.decode(\"ascii\"))\n",
        "\n",
        "# ¬°Hemos perdido la cadena de bytes subyacente original! \n",
        "# Ha sido reemplazada por la cadena de bytes subyacente para el car√°cter desconocido :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo anterior es malo y queremos evitarlo. Es mucho mejor convertir todo nuestro texto a UTF-8 tan pronto como podamos y mantenerlo en esa codificaci√≥n. El mejor momento para convertir texto no UTF-8 a UTF-8 es cuando lees archivos, de lo que hablaremos a continuaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lectura de archivos con problemas de codificaci√≥n\n",
        "\n",
        "La mayor√≠a de los archivos que encuentres probablemente estar√°n codificados con UTF-8. Esto es lo que Python espera por defecto. Esto es lo que Python espera por defecto, as√≠ que la mayor√≠a de las veces no tendr√°s problemas. Sin embargo, a veces aparecer√° un error como √©ste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# intentemos leer un archivo que no est√° en UTF-8\n",
        "file_path = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/ks-projects-201612.csv\"\n",
        "kickstarter_2016 = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intentemos detectar la codificaci√≥n correcta del archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ocupamos tener de forma local el archivo para revisarlo\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/ks-projects-201612.csv -O ks-projects-201612.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# revisar los primeros diez mil bytes para inferir la codificaci√≥n de caracteres\n",
        "with open(\"ks-projects-201612.csv\", 'rb') as rawdata:\n",
        "    result = charset_normalizer.detect(rawdata.read(10000))\n",
        "\n",
        "# verifiquemos que codificaci√≥n podr√≠a ser la correcta\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que charset_normalizer tiene un 100% de confianza en que la codificaci√≥n correcta es \"Windows-1252\". Veamos si eso es correcto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# leer el archivo con la codificaci√≥n detectada por charset_normalizer\n",
        "kickstarter_2016 = pd.read_csv(\"data/ks-projects-201612.csv\", encoding='Windows-1252')\n",
        "\n",
        "# revisemos los primeros renglones\n",
        "kickstarter_2016.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¬øQu√© pasa si la codificaci√≥n que infiere charset_normalizer no es la correcta? Como charset_normalizer es b√°sicamente un adivinador elegante, a veces inferir√° la codificaci√≥n incorrecta. Una cosa que puedes intentar es mirar m√°s o menos del archivo y ver si obtienes un resultado diferente y luego probar eso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Guardar tus archivos con codificaci√≥n UTF-8\n",
        "Por √∫ltimo, una vez que te hayas tomado la molestia de convertir tu archivo a UTF-8, probablemente querr√°s mantenerlo as√≠. La forma m√°s sencilla de hacerlo es guardar tus archivos con codificaci√≥n UTF-8. La buena noticia es que, dado que UTF-8 es la codificaci√≥n est√°ndar en Python, cuando guardes un archivo se guardar√° como UTF-8 por defecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# guardemos nuestro archivo (se guardar√° como UTF-8 por defecto)\n",
        "kickstarter_2016.to_csv(\"ks-projects-201612-utf8.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introducci√≥n de datos incoherentes\n",
        "Corrija eficazmente los errores tipogr√°ficos en sus datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cargamos los m√≥dulos y conjunto de datos que utilizaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# instalamos el m√≥dulo en caso de que no est√© precargado\n",
        "%pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "# m√≥dulos que ocupamos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# m√≥dulos √∫tiles\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import process\n",
        "import charset_normalizer\n",
        "\n",
        "# cargamos los datos\n",
        "file_path = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/pakistan_intellectual_capital.csv\"\n",
        "professors = pd.read_csv(file_path)\n",
        "professors = pd.read_csv(\"data/pakistan_intellectual_capital.csv\")\n",
        "\n",
        "# fijamos semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Realicemos preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>S#</th>\n",
              "      <th>Teacher Name</th>\n",
              "      <th>University Currently Teaching</th>\n",
              "      <th>Department</th>\n",
              "      <th>Province University Located</th>\n",
              "      <th>Designation</th>\n",
              "      <th>Terminal Degree</th>\n",
              "      <th>Graduated from</th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Area of Specialization/Research Interests</th>\n",
              "      <th>Other Information</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>Dr. Abdul Basit</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Software Engineering &amp; DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Dr. Waheed Noor</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>Dr. Junaid Baber</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Information processing, Multimedia mining</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>Dr. Maheen Bakhtyar</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NLP, Information Retrieval, Question Answering...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>Samina Azim</td>\n",
              "      <td>Sardar Bahadur Khan Women's University</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Lecturer</td>\n",
              "      <td>BS</td>\n",
              "      <td>Balochistan University of Information Technolo...</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>VLSI Electronics DLD Database</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  S#         Teacher Name  \\\n",
              "0           2   3      Dr. Abdul Basit   \n",
              "1           4   5      Dr. Waheed Noor   \n",
              "2           5   6     Dr. Junaid Baber   \n",
              "3           6   7  Dr. Maheen Bakhtyar   \n",
              "4          24  25          Samina Azim   \n",
              "\n",
              "            University Currently Teaching             Department  \\\n",
              "0               University of Balochistan  Computer Science & IT   \n",
              "1               University of Balochistan  Computer Science & IT   \n",
              "2               University of Balochistan  Computer Science & IT   \n",
              "3               University of Balochistan  Computer Science & IT   \n",
              "4  Sardar Bahadur Khan Women's University       Computer Science   \n",
              "\n",
              "  Province University Located          Designation Terminal Degree  \\\n",
              "0                 Balochistan  Assistant Professor             PhD   \n",
              "1                 Balochistan  Assistant Professor             PhD   \n",
              "2                 Balochistan  Assistant Professor             PhD   \n",
              "3                 Balochistan  Assistant Professor             PhD   \n",
              "4                 Balochistan             Lecturer              BS   \n",
              "\n",
              "                                      Graduated from   Country    Year  \\\n",
              "0                      Asian Institute of Technology  Thailand     NaN   \n",
              "1                      Asian Institute of Technology  Thailand     NaN   \n",
              "2                      Asian Institute of Technology  Thailand     NaN   \n",
              "3                      Asian Institute of Technology  Thailand     NaN   \n",
              "4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n",
              "\n",
              "           Area of Specialization/Research Interests Other Information  \n",
              "0                        Software Engineering & DBMS               NaN  \n",
              "1                                               DBMS               NaN  \n",
              "2          Information processing, Multimedia mining               NaN  \n",
              "3  NLP, Information Retrieval, Question Answering...               NaN  \n",
              "4                      VLSI Electronics DLD Database               NaN  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "professors.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Digamos que nos interesa limpiar la columna \"Country\" para asegurarnos de que no hay incoherencias en la introducci√≥n de datos. Podr√≠amos revisar cada fila a mano, por supuesto, y corregir a mano las incoherencias que encontremos. Pero hay una forma m√°s eficaz de hacerlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n",
              "       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n",
              "       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n",
              "       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n",
              "       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n",
              "       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n",
              "       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¬øQu√© incosnsistencias observamos?\n",
        "\n",
        "Lo primero que haremos ser√° pasar los textos a min√∫sculas y eliminar los espacios adicionales al inicio y final de los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convertir a min√∫sculas\n",
        "professors['Country'] = professors['Country'].str.lower()\n",
        "\n",
        "# remover espacios en blanco al inicio y final\n",
        "professors['Country'] = professors['Country'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>S#</th>\n",
              "      <th>Teacher Name</th>\n",
              "      <th>University Currently Teaching</th>\n",
              "      <th>Department</th>\n",
              "      <th>Province University Located</th>\n",
              "      <th>Designation</th>\n",
              "      <th>Terminal Degree</th>\n",
              "      <th>Graduated from</th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Area of Specialization/Research Interests</th>\n",
              "      <th>Other Information</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>Dr. Abdul Basit</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Software Engineering &amp; DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Dr. Waheed Noor</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>Dr. Junaid Baber</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Information processing, Multimedia mining</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>Dr. Maheen Bakhtyar</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NLP, Information Retrieval, Question Answering...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>Samina Azim</td>\n",
              "      <td>Sardar Bahadur Khan Women's University</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Lecturer</td>\n",
              "      <td>BS</td>\n",
              "      <td>Balochistan University of Information Technolo...</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>VLSI Electronics DLD Database</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  S#         Teacher Name  \\\n",
              "0           2   3      Dr. Abdul Basit   \n",
              "1           4   5      Dr. Waheed Noor   \n",
              "2           5   6     Dr. Junaid Baber   \n",
              "3           6   7  Dr. Maheen Bakhtyar   \n",
              "4          24  25          Samina Azim   \n",
              "\n",
              "            University Currently Teaching             Department  \\\n",
              "0               University of Balochistan  Computer Science & IT   \n",
              "1               University of Balochistan  Computer Science & IT   \n",
              "2               University of Balochistan  Computer Science & IT   \n",
              "3               University of Balochistan  Computer Science & IT   \n",
              "4  Sardar Bahadur Khan Women's University       Computer Science   \n",
              "\n",
              "  Province University Located          Designation Terminal Degree  \\\n",
              "0                 Balochistan  Assistant Professor             PhD   \n",
              "1                 Balochistan  Assistant Professor             PhD   \n",
              "2                 Balochistan  Assistant Professor             PhD   \n",
              "3                 Balochistan  Assistant Professor             PhD   \n",
              "4                 Balochistan             Lecturer              BS   \n",
              "\n",
              "                                      Graduated from   Country    Year  \\\n",
              "0                      Asian Institute of Technology  Thailand     NaN   \n",
              "1                      Asian Institute of Technology  Thailand     NaN   \n",
              "2                      Asian Institute of Technology  Thailand     NaN   \n",
              "3                      Asian Institute of Technology  Thailand     NaN   \n",
              "4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n",
              "\n",
              "           Area of Specialization/Research Interests Other Information  \n",
              "0                        Software Engineering & DBMS               NaN  \n",
              "1                                               DBMS               NaN  \n",
              "2          Information processing, Multimedia mining               NaN  \n",
              "3  NLP, Information Retrieval, Question Answering...               NaN  \n",
              "4                      VLSI Electronics DLD Database               NaN  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otra forma de convertir a min√∫sculas es utilizando la funci√≥n apply y str.lower.\n",
        "\n",
        "professors['Country'] = professors.Country.apply(str.capitalize)\n",
        "professors.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizar la concordancia difusa para corregir la introducci√≥n de datos incoherentes\n",
        "Muy bien, echemos otro vistazo a la columna \"Country\" y veamos si hay que limpiar m√°s datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Australia', 'Austria', 'Canada', 'China', 'Finland', 'France',\n",
              "       'Germany', 'Greece', 'Hongkong', 'Ireland', 'Italy', 'Japan',\n",
              "       'Macau', 'Malaysia', 'Mauritius', 'Netherland', 'New zealand',\n",
              "       'Norway', 'Pakistan', 'Portugal', 'Russian federation',\n",
              "       'Saudi arabia', 'Scotland', 'Singapore', 'South korea',\n",
              "       'Southkorea', 'Spain', 'Sweden', 'Thailand', 'Turkey', 'Uk',\n",
              "       'Urbana', 'Usa', 'Usofa'], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parece que hay otra incoherencia: \"southkorea\" y \"south korea\" deber√≠an ser lo mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a utilizar el paquete fuzzywuzzy para que nos ayude a identificar las cadenas que m√°s se parecen entre s√≠. Este conjunto de datos es lo suficientemente peque√±o como para que pudi√©ramos corregir los errores a mano, pero este m√©todo no es muy adecuado. (¬øLe gustar√≠a corregir mil errores a mano? ¬øY diez mil? Automatizar las cosas lo antes posible suele ser una buena idea. Adem√°s, es divertido).\n",
        "\n",
        "**Coincidencia difusa**: proceso que consiste en encontrar autom√°ticamente cadenas de texto muy similares a la cadena objetivo. En general, una cadena se considera \"m√°s parecida\" a otra cuantos menos caracteres tendr√≠a que cambiar si transformara una cadena en otra. As√≠, \"apple\" y \"snapple\" est√°n a dos cambios de distancia (a√±adir \"s\" y \"n\"), mientras que \"in\" y \"on\" est√°n a un cambio (sustituir \"i\" por \"o\"). No siempre podr√° confiar al 100% en la concordancia difusa, pero normalmente acabar√° ahorr√°ndole al menos un poco de tiempo.\n",
        "\n",
        "Fuzzywuzzy devuelve un cociente dadas dos cadenas. Cuanto m√°s se acerque el cociente a 100, menor ser√° la distancia de edici√≥n entre las dos cadenas. Aqu√≠, vamos a obtener las diez cadenas de nuestra lista de pa√≠ses que tienen la distancia m√°s cercana a \"south korea\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('South korea', 100),\n",
              " ('Southkorea', 48),\n",
              " ('Saudi arabia', 43),\n",
              " ('Norway', 35),\n",
              " ('Ireland', 33),\n",
              " ('Portugal', 32),\n",
              " ('Singapore', 30),\n",
              " ('Netherland', 29),\n",
              " ('Macau', 25),\n",
              " ('Usofa', 25)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtener las 10 coincidencias m√°s cercanas a \"south korea\"\n",
        "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "\n",
        "# revisemos\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que dos de los elementos de las ciudades est√°n muy cerca de \"south korea\": \"south korea\" y \"southkorea\". Sustituyamos todas las filas de nuestra columna \"Country\" que tengan una proporci√≥n > 47 por \"south korea\".\n",
        "\n",
        "Para ello, vamos a escribir una funci√≥n. (Es una buena idea escribir una funci√≥n de prop√≥sito general que puedas reutilizar si crees que vas a tener que hacer una tarea espec√≠fica m√°s de una o dos veces. Esto evita que tengas que copiar y pegar c√≥digo con demasiada frecuencia, lo que ahorra tiempo y puede ayudar a evitar errores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# funci√≥n para remplazar los textos de la columna indicada con base al nivel de coincidencia indicado\n",
        "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
        "    # obtenemos los valores √∫nicos\n",
        "    strings = df[column].unique()\n",
        "    \n",
        "    # obtenemos los 10 valores m√°s cercano a nuestra cadena\n",
        "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
        "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "\n",
        "    # filtramos solo las coincidencias con un valore de similitud > min_ratio\n",
        "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
        "\n",
        "    # obtenemos los registros (renglones) de las coincidencias m√°s cercanas en el conjunto de datos\n",
        "    rows_with_matches = df[column].isin(close_matches)\n",
        "\n",
        "    # remplazamos los textos con el texto correcto\n",
        "    df.loc[rows_with_matches, column] = string_to_match\n",
        "    \n",
        "    # mostramos mensaje de que hemos finalizado\n",
        "    print(\"¬°Listo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Listo!\n"
          ]
        }
      ],
      "source": [
        "# usamos la funci√≥n para remplazar coincidencias cercanas a \"south korea\" con \"south korea\"\n",
        "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Revisamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Australia', 'Austria', 'Canada', 'China', 'Finland', 'France',\n",
              "       'Germany', 'Greece', 'Hongkong', 'Ireland', 'Italy', 'Japan',\n",
              "       'Macau', 'Malaysia', 'Mauritius', 'Netherland', 'New zealand',\n",
              "       'Norway', 'Pakistan', 'Portugal', 'Russian federation',\n",
              "       'Saudi arabia', 'Scotland', 'Singapore', 'Spain', 'Sweden',\n",
              "       'Thailand', 'Turkey', 'Uk', 'Urbana', 'Usa', 'Usofa',\n",
              "       'south korea'], dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
